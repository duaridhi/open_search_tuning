{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147dc301-90d1-4d73-b783-bcdee6987d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up connection\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "OPENSEARCH_HOST = \"opensearch-node\"\n",
    "OPENSEARCH_PORT = 9200\n",
    "INDEX_NAME = \"ir-dataset\"\n",
    "\n",
    "# ----------------------------\n",
    "# OpenSearch client\n",
    "# ----------------------------\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": OPENSEARCH_HOST, \"port\": OPENSEARCH_PORT}],\n",
    "    http_compress=True,\n",
    "    use_ssl=False,\n",
    "    verify_certs=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3341b-2b1c-4833-a97b-0b4a8160d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create an index\n",
    "\n",
    "\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\"type\": \"keyword\"},\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "client.info()\n",
    "client.indices.create(index=INDEX_NAME, body=index_body)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e58d7-6b68-4a38-a573-272589af3468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Index a single document to try \n",
    "\n",
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"msmarco-passage/dev/small\")\n",
    "\n",
    "doc = next(dataset.docs_iter())\n",
    "print(doc)\n",
    "\n",
    "client.index(\n",
    "    index=INDEX_NAME,\n",
    "    id=doc.doc_id,\n",
    "    body={\n",
    "        \"doc_id\": doc.doc_id,\n",
    "        \"text\": doc.text\n",
    "    },\n",
    "    refresh=True   # IMPORTANT for immediate search\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d736e6e0-8bc2-4c4a-bbfb-01f6f28dabb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qrels: 7437\n",
      "Docs: 8841823\n",
      "Queries: 6980\n"
     ]
    }
   ],
   "source": [
    "client.count(index=INDEX_NAME)\n",
    "print(\"Qrels:\", dataset.qrels_count())\n",
    "print(\"Docs:\", dataset.docs_count())\n",
    "print(\"Queries:\", dataset.queries_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04038a78-8a50-4394-8bce-06a47520faf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing documents:  88%|████████▊ | 8841000/10000000 [31:17<04:06, 4708.43it/s]  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input source data size too small",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m error_count = \u001b[32m0\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=MAX_DOCS, desc=\u001b[33m\"\u001b[39m\u001b[33mIndexing documents\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msuccess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhelpers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstreaming_bulk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_docs_bulk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msuccess\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdoc_count\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/opensearchpy/helpers/actions.py:326\u001b[39m, in \u001b[36mstreaming_bulk\u001b[39m\u001b[34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, *args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03mStreaming bulk consumes actions from the iterable passed in and yields\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03mresults per action. For non-streaming usecases use\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    322\u001b[39m \u001b[33;03m:arg ignore_status: list of HTTP status code that you want to ignore\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    324\u001b[39m actions = \u001b[38;5;28mmap\u001b[39m(expand_action_callback, actions)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbulk_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbulk_actions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_chunk_actions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_chunk_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mserializer\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mto_retry\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/opensearchpy/helpers/actions.py:167\u001b[39m, in \u001b[36m_chunk_actions\u001b[39m\u001b[34m(actions, chunk_size, max_chunk_bytes, serializer)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[33;03mSplit actions into chunks by number or size, serialize them into strings in\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[33;03mthe process.\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    164\u001b[39m chunker = _ActionChunker(\n\u001b[32m    165\u001b[39m     chunk_size=chunk_size, max_chunk_bytes=max_chunk_bytes, serializer=serializer\n\u001b[32m    166\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mret\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mret\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mindex_docs_bulk\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     27\u001b[39m success_count = \u001b[32m0\u001b[39m\n\u001b[32m     29\u001b[39m docs_iter = dataset.docs_iter()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocs_iter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_DOCS\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/ir_datasets/indices/lz4_pickle.py:64\u001b[39m, in \u001b[36mLz4PickleIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mself\u001b[39m.bin.seek(new_pos) \u001b[38;5;66;03m# this seek is smart -- if alrady in buffer, skips to that point\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mself\u001b[39m.next_index = \u001b[38;5;28mself\u001b[39m.slice.start\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m result = \u001b[43m_read_next\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlookup\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_doc_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m.next_index += \u001b[32m1\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m.slice = \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28mself\u001b[39m.slice.start + (\u001b[38;5;28mself\u001b[39m.slice.step \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m), \u001b[38;5;28mself\u001b[39m.slice.stop, \u001b[38;5;28mself\u001b[39m.slice.step)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/ir_datasets/indices/lz4_pickle.py:20\u001b[39m, in \u001b[36m_read_next\u001b[39m\u001b[34m(f, data_cls)\u001b[39m\n\u001b[32m     18\u001b[39m content_length = \u001b[38;5;28mint\u001b[39m.from_bytes(f.read(\u001b[32m4\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m content = f.read(content_length)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m content = \u001b[43mlz4\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m content = pickle.loads(content)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data_cls(*content)\n",
      "\u001b[31mValueError\u001b[39m: Input source data size too small"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch, helpers\n",
    "import ir_datasets\n",
    "import time\n",
    "dataset = ir_datasets.load(\"msmarco-passage/dev/small\")\n",
    "from opensearchpy.helpers import bulk\n",
    "from tqdm import tqdm\n",
    "MAX_DOCS = 10_000_000\n",
    "BATCH_SIZE = 1000   # safe value for laptops\n",
    "\n",
    "\n",
    "\n",
    "def index_docs():\n",
    "    for doc in dataset.docs_iter():\n",
    "        client.index(\n",
    "        index=INDEX_NAME,\n",
    "        id=doc.doc_id,\n",
    "        body={\n",
    "            \"doc_id\": doc.doc_id,\n",
    "            \"text\": doc.text\n",
    "        },\n",
    "        refresh=True   # IMPORTANT for immediate search\n",
    ")\n",
    "\n",
    "def index_docs_bulk():\n",
    "    actions = []\n",
    "    error_count = 0\n",
    "    success_count = 0\n",
    "\n",
    "    docs_iter = dataset.docs_iter()\n",
    "\n",
    "    for i, doc in enumerate(docs_iter):\n",
    "        if i >= MAX_DOCS:\n",
    "            break\n",
    "        yield {\n",
    "            \"_index\": INDEX_NAME,\n",
    "            \"_id\": doc.doc_id,\n",
    "            \"_source\": {\n",
    "                \"doc_id\": doc.doc_id,\n",
    "                \"text\": doc.text\n",
    "            }\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# BULK INDEX WITH PROGRESS\n",
    "# -----------------------------\n",
    "start_time = time.time()\n",
    "doc_count = 0\n",
    "error_count = 0\n",
    "\n",
    "with tqdm(total=MAX_DOCS, desc=\"Indexing documents\") as pbar:\n",
    "    for success, info in helpers.streaming_bulk(\n",
    "        client,\n",
    "        index_docs_bulk(),\n",
    "        chunk_size=BATCH_SIZE,\n",
    "        request_timeout=120,\n",
    "    ):\n",
    "        if success:\n",
    "            doc_count += 1\n",
    "        else:\n",
    "            error_count += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# -----------------------------\n",
    "# FINALIZE\n",
    "# -----------------------------\n",
    "client.indices.put_settings(\n",
    "    index=INDEX_NAME,\n",
    "    body={\"index\": {\"refresh_interval\": \"1s\"}}\n",
    ")\n",
    "client.indices.refresh(index=INDEX_NAME)\n",
    "\n",
    "# -----------------------------\n",
    "# STATS\n",
    "# -----------------------------\n",
    "elapsed = end_time - start_time\n",
    "rate = doc_count / elapsed\n",
    "\n",
    "count_in_index = client.count(index=INDEX_NAME)[\"count\"]\n",
    "\n",
    "print(\"\\n====== INGESTION COMPLETE ======\")\n",
    "print(f\"Documents indexed: {doc_count}\")\n",
    "print(f\"Errors: {error_count}\")\n",
    "print(f\"Elapsed time: {elapsed:.2f} seconds\")     \n",
    "print(f\"Indexing rate: {rate:.2f} docs/sec\")\n",
    "print(f\"Docs in index: {count_in_index}\")\n",
    "\n",
    "client.transport.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6335439-d7f9-4f20-8ce9-fb1149bedb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels =dataset.qrels_iter()\n",
    "dataset.qrels_count\n",
    "\n",
    "#print(next(qrels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce346b-e60c-4780-a649-b94cfbac445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bulk index all the documents in the dataset \n",
    "\n",
    "import json\n",
    "from opensearchpy import OpenSearch, helpers\n",
    "\n",
    "# ----------------------------\n",
    "# Bulk index\n",
    "# ----------------------------\n",
    "print(\"Indexing documents...\")\n",
    "helpers.bulk(\n",
    "    client,\n",
    "    index_docs_bulk(),\n",
    "    chunk_size=100,\n",
    "    request_timeout=120\n",
    ")\n",
    "\n",
    "print(\"✅ Indexing complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda8b20-0148-49ca-857e-df1f8a65e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.count(index=INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5eb88c-42e3-4ad6-b559-e9a0dc5118a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ranx\n",
    "from ranx import Qrels, Run, evaluate\n",
    "\n",
    "qrels = Qrels.from_ir_datasets(dataset)\n",
    "run = Run.from_dict({\n",
    "    \"q1\": {\"doc1\": 1.0, \"doc2\": 0.8}\n",
    "})\n",
    "\n",
    "metrics = evaluate(qrels, run, [\"precision@10\", \"recall@10\"])\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1540244e-f6f4-469b-954c-f58d8d08c635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
