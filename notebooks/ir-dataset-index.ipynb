{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcee4f-292f-4ae1-9c9b-449c55289e50",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Dataset configuration: MS MARCO passage training set with 200-way split\n",
    "IR_DATASET_NAME=\"msmarco-passage/train/split200-train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17717ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenSearch instance\n",
    "import open_search_connect\n",
    "from open_search_connect import connect\n",
    "client = connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40b78b-88e9-48e6-b017-b8af59ec15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and check the current document count in the index\n",
    "dataset = ir_datasets.load(IR_DATASET_NAME)\n",
    "\n",
    "client.count(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c368aa-b04e-4dd8-a1fc-07551a98521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OpenSearch index with document mappings (doc_id as keyword, text as full-text)\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"knn\": True\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\"type\": \"keyword\"},\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"english\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "client.info()\n",
    "client.indices.create(index=INDEX_NAME, body=index_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f3ccb-9d70-4ed9-8cb0-2c46df137166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test indexing: index a single document to verify the connection and mapping work\n",
    "import ir_datasets\n",
    "dataset = ir_datasets.load(IR_DATASET_NAME)\n",
    "\n",
    "doc = next(dataset.docs_iter())\n",
    "print(doc)\n",
    "\n",
    "client.index(\n",
    "    index=INDEX_NAME,\n",
    "    id=doc.doc_id,\n",
    "    body={\n",
    "        \"doc_id\": doc.doc_id,\n",
    "        \"text\": doc.text\n",
    "    },\n",
    "    refresh=True   # IMPORTANT: ensures document is immediately searchable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75915a-29fc-4766-9579-64a15b5f9d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6fb76-2494-4137-ae5c-177593696ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk index all documents from the dataset with progress tracking\n",
    "from opensearchpy import OpenSearch, helpers\n",
    "import ir_datasets\n",
    "import time\n",
    "dataset = ir_datasets.load(IR_DATASET_NAME)\n",
    "from opensearchpy.helpers import bulk\n",
    "from tqdm import tqdm\n",
    "MAX_DOCS = 10_000_000\n",
    "BATCH_SIZE = 1000   # safe value for laptops\n",
    "\n",
    "\n",
    "def index_docs():\n",
    "    \"\"\"Single-document indexing (slower but simpler)\"\"\"\n",
    "    for doc in dataset.docs_iter():\n",
    "        client.index(\n",
    "        index=INDEX_NAME,\n",
    "        id=doc.doc_id,\n",
    "        body={\n",
    "            \"doc_id\": doc.doc_id,\n",
    "            \"text\": doc.text\n",
    "        },\n",
    "        refresh=True   # IMPORTANT for immediate search\n",
    ")\n",
    "\n",
    "def index_docs_bulk():\n",
    "    \"\"\"Generator function that yields documents in bulk format for streaming_bulk\"\"\"\n",
    "    docs_iter = dataset.docs_iter()\n",
    "\n",
    "    for i, doc in enumerate(docs_iter):\n",
    "        if i >= MAX_DOCS:\n",
    "            break\n",
    "        yield {\n",
    "            \"_index\": INDEX_NAME,\n",
    "            \"_id\": doc.doc_id,\n",
    "            \"_source\": {\n",
    "                \"doc_id\": doc.doc_id,\n",
    "                \"text\": doc.text\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Bulk index with streaming to handle large datasets efficiently\n",
    "start_time = time.time()\n",
    "doc_count = 0\n",
    "error_count = 0\n",
    "\n",
    "with tqdm(total=MAX_DOCS, desc=\"Indexing documents\") as pbar:\n",
    "    for success, info in helpers.streaming_bulk(\n",
    "        client,\n",
    "        index_docs_bulk(),\n",
    "        chunk_size=BATCH_SIZE,\n",
    "        request_timeout=120,\n",
    "    ):\n",
    "        if success:\n",
    "            doc_count += 1\n",
    "        else:\n",
    "            error_count += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Re-enable refresh interval and refresh index after bulk load\n",
    "client.indices.put_settings(\n",
    "    index=INDEX_NAME,\n",
    "    body={\"index\": {\"refresh_interval\": \"1s\"}}\n",
    ")\n",
    "client.indices.refresh(index=INDEX_NAME)\n",
    "\n",
    "# Print final statistics about the indexing operation\n",
    "elapsed = end_time - start_time\n",
    "rate = doc_count / elapsed\n",
    "\n",
    "count_in_index = client.count(index=INDEX_NAME)[\"count\"]\n",
    "\n",
    "print(\"\\n====== INGESTION COMPLETE ======\")\n",
    "print(f\"Documents indexed: {doc_count}\")\n",
    "print(f\"Errors: {error_count}\")\n",
    "print(f\"Elapsed time: {elapsed:.2f} seconds\")     \n",
    "print(f\"Indexing rate: {rate:.2f} docs/sec\")\n",
    "print(f\"Docs in index: {count_in_index}\")\n",
    "\n",
    "client.transport.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26acfba0-f244-42eb-8112-3bbc492652fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0434ae-9dcd-47d2-a7dc-5a5f2e1498a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative bulk indexing approach (simpler but less progress feedback than streaming_bulk)\n",
    "import json\n",
    "from opensearchpy import OpenSearch, helpers\n",
    "\n",
    "print(\"Indexing documents...\")\n",
    "helpers.bulk(\n",
    "    client,\n",
    "    index_docs_bulk(),\n",
    "    chunk_size=100,\n",
    "    request_timeout=120\n",
    ")\n",
    "\n",
    "print(\"âœ… Indexing complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
